import aiohttp
import asyncio
import time
from urllib.parse import urlparse, urlunparse
import argparse

# Load the wordlist from a file
def load_wordlist(file_path):
    with open(file_path, 'r') as file:
        return [line.strip() for line in file]

# Generate URLs by inserting words from the wordlist into different positions
def generate_urls(base_url, wordlist):
    parsed_url = urlparse(base_url)
    base_path_parts = parsed_url.path.strip('/').split('/')
    
    urls = []
    for word in wordlist:
        for i in range(len(base_path_parts) + 1):
            new_path_parts = base_path_parts[:i] + [word] + base_path_parts[i:]
            new_path = '/' + '/'.join(new_path_parts)
            new_url = urlunparse((parsed_url.scheme, parsed_url.netloc, new_path, '', '', ''))
            urls.append(new_url)
    
    return urls

# Fuzz the generated URLs asynchronously
async def fuzz_url(session, original_response, url, original_length, original_time):
    try:
        start_time = time.time()
        async with session.get(url) as response:
            response_text = await response.text()
            response_length = len(response_text)
            response_time = time.time() - start_time
            
            if response_length != original_length or response_time != original_time:
                return {
                    'url': url,
                    'status_code': response.status,
                    'length_diff': response_length - original_length,
                    'time_diff': response_time - original_time
                }
    except Exception as e:
        print(f"Error with URL {url}: {e}")
    return None

async def fuzz_urls(original_url, urls):
    async with aiohttp.ClientSession() as session:
        start_time = time.time()
        async with session.get(original_url) as original_response:
            original_text = await original_response.text()
            original_length = len(original_text)
            original_time = time.time() - start_time
            
            tasks = [fuzz_url(session, original_response, url, original_length, original_time) for url in urls]
            results = await asyncio.gather(*tasks)
    
    return [result for result in results if result]

# Main function to run the fuzzing tool
def main():
    # Setup argument parser
    parser = argparse.ArgumentParser(description='URL Fuzzing Tool')
    parser.add_argument('-w', '--wordlist', required=True, help='Path to the wordlist file')
    parser.add_argument('-u', '--url', required=True, help='Target URL to fuzz')

    # Parse the arguments
    args = parser.parse_args()

    # Load the wordlist and target URL
    wordlist = load_wordlist(args.wordlist)
    target_url = args.url
    
    # Generate URLs and fuzz them
    urls_to_fuzz = generate_urls(target_url, wordlist)
    results = asyncio.run(fuzz_urls(target_url, urls_to_fuzz))
    
    # Print the results
    for result in results:
        print(f"URL: {result['url']}")
        print(f"Status Code: {result['status_code']}")
        print(f"Length Difference: {result['length_diff']}")
        print(f"Time Difference: {result['time_diff']}")
        print('--------------------')

if __name__ == "__main__":
    main()
